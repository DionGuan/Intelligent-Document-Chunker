# main.py
import os
import sys
from typing import Optional, List

# ç¡®ä¿å¯¼å…¥è·¯å¾„æ­£ç¡®
# å‡è®¾ semantic_chunker_local.py å’Œ document_loader_local.py åœ¨åŒä¸€ç›®å½•
from langchain_experimental.text_splitter import SemanticChunker
from langchain_community.embeddings import HuggingFaceEmbeddings
from semantic_chunker_local import LocalSemanticChunker
# å¯¼å…¥æ–°çš„ PDF å¤„ç†å‡½æ•°
from document_loader_local import load_txt_file, load_pdf_and_extract_md

def get_file_path() -> Optional[str]:
    """èŽ·å–æ–‡ä»¶è·¯å¾„"""
    print("\nðŸ“‚ è¯·é€‰æ‹©è¦åˆ‡åˆ†çš„æ–‡æ¡£ï¼ˆæ”¯æŒ .txt æˆ– .pdfï¼‰")
    print("ðŸ‘‰ æ–¹æ³•1ï¼šç›´æŽ¥æŠŠæ–‡ä»¶æ‹–å…¥ç»ˆç«¯çª—å£")
    print("ðŸ‘‰ æ–¹æ³•2ï¼šæ‰‹åŠ¨è¾“å…¥å®Œæ•´è·¯å¾„")
    print("-" * 60)
    try:
        # åŽ»é™¤ç”¨æˆ·å¯èƒ½è¯¯åŠ çš„å¼•å·
        path = input("è¯·è¾“å…¥æ–‡ä»¶è·¯å¾„: ").strip().strip('"').strip("'")
        # è§„èŒƒåŒ–è·¯å¾„åˆ†éš”ç¬¦
        path = os.path.normpath(path)
        if os.path.exists(path) and os.path.isfile(path):
            return path
        else:
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸æ˜¯æœ‰æ•ˆæ–‡ä»¶: {path}")
            return None
    except KeyboardInterrupt:
        print("\nðŸ‘‹ ç”¨æˆ·å–æ¶ˆ")
        sys.exit(0)
    except Exception as e:
        print(f"âŒ è¾“å…¥é”™è¯¯: {e}")
        return None

def save_chunks_to_output_folder(chunks: List[str], original_file_path: str):
    """
    å°† chunks ä¿å­˜åˆ°ä¸ŽåŽŸå§‹æ–‡ä»¶åŒçº§çš„ output æ–‡ä»¶å¤¹å†…ã€‚
    æ–‡ä»¶åæ ¼å¼: åŽŸå§‹æ–‡ä»¶å_without_ext.chunk.1.txt, .chunk.2.txt, ...
    åŒæ—¶åˆ›å»ºä¸€ä¸ªåˆå¹¶çš„txtæ–‡ä»¶ï¼ŒåŒ…å«æ‰€æœ‰chunk
    """
    if not chunks:
        print("âš ï¸  æ²¡æœ‰ chunks éœ€è¦ä¿å­˜ã€‚")
        return

    base_dir = os.path.dirname(original_file_path)
    original_name_without_ext = os.path.splitext(os.path.basename(original_file_path))[0]
    output_dir = os.path.join(base_dir, "output", original_name_without_ext) # ä¿å­˜åœ¨ output/å­æ–‡ä»¶å¤¹å/ ä¸‹

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)

    # ä¿å­˜æ¯ä¸ªchunkåˆ°å•ç‹¬æ–‡ä»¶
    saved_files = []
    for i, chunk in enumerate(chunks, start=1):
        filename = f"{original_name_without_ext}.chunk.{i}.txt"
        file_path = os.path.join(output_dir, filename)
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(chunk)
            saved_files.append(file_path)
            print(f"ðŸ’¾ å·²ä¿å­˜ Chunk {i} åˆ°: {file_path}")
        except Exception as e:
            print(f"âŒ ä¿å­˜ Chunk {i} å¤±è´¥: {e}")

    # åˆ›å»ºåˆå¹¶çš„txtæ–‡ä»¶
    combined_filename = f"{original_name_without_ext}.all_chunks.txt"
    combined_file_path = os.path.join(output_dir, combined_filename)
    
    try:
        with open(combined_file_path, 'w', encoding='utf-8') as f:
            for i, chunk in enumerate(chunks, start=1):
                f.write(f"=== Chunk {i} ===\n{chunk}\n\n")
        print(f"âœ… å·²åˆ›å»ºåˆå¹¶æ–‡ä»¶: {combined_file_path}")
    except Exception as e:
        print(f"âŒ åˆ›å»ºåˆå¹¶æ–‡ä»¶å¤±è´¥: {e}")

    print(f"âœ… æ€»å…±ä¿å­˜äº† {len(saved_files)} ä¸ª chunks åˆ° {output_dir}")
def main():
    file_path = get_file_path()
    if not file_path:
        return

    print(f"\nðŸ“‚ æ­£åœ¨åŠ è½½æ–‡ä»¶: {file_path}")
    ext = os.path.splitext(file_path)[1].lower()

    text = ""
    if ext == ".txt":
        text = load_txt_file(file_path)
        if not text:
             print("âŒ æœªèƒ½ä»Ž TXT æ–‡ä»¶ä¸­è¯»å–æœ‰æ•ˆæ–‡æœ¬ã€‚")
             return
    elif ext == ".pdf":
        # --- å¼ºåˆ¶è°ƒç”¨ç®€åŒ–åŽçš„ MinerU å¹¶è¯»å– Markdown ---
        text = load_pdf_and_extract_md(file_path)
        if not text:
             print("âŒ æœªèƒ½é€šè¿‡ MinerU ä»Ž PDF ä¸­æå–æœ‰æ•ˆæ–‡æœ¬ã€‚")
             return
        print(f"ðŸ“– æå–å¹¶æ¸…ç†åŽçš„æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
    else:
        print("âŒ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œè¯·æä¾› .txt æˆ– .pdf æ–‡ä»¶ã€‚")
        return

    if len(text) == 0:
         print("âš ï¸  æå–çš„æ–‡æœ¬ä¸ºç©ºï¼Œå°†ä¸è¿›è¡Œåˆ‡åˆ†ã€‚")
         return


    # --- è¯­ä¹‰åˆ‡åˆ† ---
    print("\nâœ‚ï¸  æ­£åœ¨åˆå§‹åŒ–è¯­ä¹‰åˆ‡åˆ†å™¨...")
    # @todo æœªæ¥å¯åœ¨æ­¤å¤„æ ¹æ®æ–‡ä»¶ç±»åž‹æˆ–ç”¨æˆ·è¾“å…¥è°ƒæ•´å‚æ•°
    try:
        chunker = LocalSemanticChunker(
            embedding_model_name="sentence-transformers/all-MiniLM-L6-v2", # @todo æ¢ qwen3-embedding
            breakpoint_threshold_type="percentile",
            breakpoint_threshold_amount=95, # @todo å¯åŠ¨æ€è°ƒæ•´
            device="cpu" # @todo å¯æ ¹æ®çŽ¯å¢ƒè°ƒæ•´ä¸º "cuda"
        )
    except Exception as e:
         print(f"âŒ åˆå§‹åŒ–è¯­ä¹‰åˆ‡åˆ†å™¨å¤±è´¥: {e}")
         import traceback
         traceback.print_exc()
         return

    print("ðŸ§  æ­£åœ¨è¿›è¡Œè¯­ä¹‰åˆ‡åˆ†...")
    try:
        chunks = chunker.split_text(text)
        print(f"ðŸŽ¯ è¯­ä¹‰åˆ‡åˆ†å®Œæˆï¼Œæ€»å…±åˆ‡åˆ†å‡º {len(chunks)} ä¸ªå—ã€‚")
    except Exception as e:
         print(f"âŒ è¯­ä¹‰åˆ‡åˆ†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
         import traceback
         traceback.print_exc() # æ‰“å°è¯¦ç»†å †æ ˆä¿¡æ¯ï¼Œä¾¿äºŽè°ƒè¯•
         return # åˆ‡åˆ†å¤±è´¥åˆ™ä¸ç»§ç»­

    if not chunks:
         print("âš ï¸  è¯­ä¹‰åˆ‡åˆ†ç»“æžœä¸ºç©ºã€‚")
         return

    # --- ä¿å­˜ç»“æžœ ---
    print("\nðŸ’¾ æ­£åœ¨ä¿å­˜åˆ‡åˆ†ç»“æžœ...")
    save_chunks_to_output_folder(chunks, file_path)

    # --- (å¯é€‰) åœ¨ç»ˆç«¯æ‰“å°é¢„è§ˆ ---
    print("\nðŸ‘€ åˆ‡åˆ†ç»“æžœé¢„è§ˆ (å‰ 3 ä¸ªå—):")
    for i in range(min(3, len(chunks))):
        print(f"\n--- Chunk {i+1} ({len(chunks[i])} å­—ç¬¦) ---")
        preview_text = chunks[i][:300] + "..." if len(chunks[i]) > 300 else chunks[i]
        print(preview_text)


if __name__ == "__main__":
    main()