# # ğŸ§© Intelligent Document Chunker

A lightweight knowledge base component for intelligent document segmentation using **semantic chunking techniques**.  
It supports both PDF and TXT inputs, performing structure-aware segmentation to enhance downstream knowledge retrieval and embedding quality.

---

## ğŸ“˜ Introduction

**Intelligent Document Chunker** is designed to automatically divide long-form documents (`.txt` or `.pdf`) into smaller, semantically meaningful chunks.  
It integrates robust text extraction (via `OCR like MinerU`) with semantic embedding models (`sentence-transformers`) to produce content-aware document partitions ideal for LLM preprocessing, vector database ingestion, or knowledge base construction.

---

## âœ¨ Features

- âœ… Supports both **TXT** and **PDF** input formats  
- ğŸ§  Performs **semantic segmentation** using `sentence-transformers`  
- ğŸ“„ Extracts and cleans text from PDFs using **OCR like MinerU** (or compatible tools)  
- ğŸ’¾ Exports chunked text as individual `.txt` files and a combined summary file  
- ğŸ“‚ Generates a structured output folder for easy downstream processing  

---

## âš™ï¸ Environment Requirements

- Python **3.8+**
- Recommended: Virtual environment for dependency isolation

---

## ğŸš€ Quick Start

### 1ï¸âƒ£ Clone or Download the Repository

```bash
git clone <your-repo-url>
# or simply download and extract the project archive
```

---

### 2ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

> ğŸ’¡ Tip:  
> It is strongly recommended to create and activate a virtual environment before installation:
> ```bash
> python -m venv .venv
> .venv\Scripts\activate    # Windows
> source .venv/bin/activate # macOS / Linux
> ```

---

### 3ï¸âƒ£ Directory Structure

```bash
Chunk/
â”œâ”€â”€ main.py
â”œâ”€â”€ semantic_chunker_local.py
â”œâ”€â”€ document_loader_local.py
â”œâ”€â”€ test_file.pdf              # Example file for testing
â””â”€â”€ output/                    # Automatically generated output
    â””â”€â”€ test_file/
        â”œâ”€â”€ test_file.chunk.1.txt
        â”œâ”€â”€ test_file.chunk.2.txt
        â”œâ”€â”€ ...
        â”œâ”€â”€ test_file.all_chunks.txt
        â””â”€â”€ auto/
            â”œâ”€â”€ test_file.md
            â”œâ”€â”€ test_file_content_list.json
            â”œâ”€â”€ test_file_layout.pdf
            â”œâ”€â”€ test_file_middle.json
            â”œâ”€â”€ test_file_model.json
            â”œâ”€â”€ test_file_origin.pdf
            â”œâ”€â”€ test_file_span.pdf
            â””â”€â”€ images/         # OCR screenshots
                â”œâ”€â”€ img1.jpg
                â”œâ”€â”€ img2.jpg
                â””â”€â”€ ...
```

---

### 4ï¸âƒ£ Run the Application

Execute the following command in the terminal:

```bash
python main.py
```

You will be prompted to select a file path (TXT or PDF).  
The program will then automatically extract, clean, segment, and save the results under the `/output` directory.

---

## ğŸ§© Workflow Overview

```
TXT / PDF
   â†“
Text Extraction (OCR like MinerU for PDF)
   â†“
Markdown Cleaning
   â†“
Semantic Embedding & Chunk Detection
   â†“
Chunked Output Files (.txt)
```

---

## ğŸ§  Typical Use Cases

- Preprocessing large documents for **LLM training or fine-tuning**  
- Preparing content for **semantic search** or **vector databases** (e.g., FAISS, Milvus)  
- Structuring long documents for **knowledge retrieval systems**  
- Academic, legal, or technical **document analysis pipelines**

---

## â­ Todo List
- [x] OCR recognition optimization based on MinerU has been completed
- [ ] Implement more standardized preprocessing for `Markdown` and `JSON` files  
- [ ] Develop more detailed sliding window strategies tailored to the recognition of different document types.
- [ ] Integrate powerful Qwen3-embedding and Qwen3-reranker models to build a complete Retrieval-Augmented Generation (RAG) system.
- [ ] Integrate the system into an open-source AI platform.

---

## ğŸ§ª Example Output (Terminal)

```
ğŸ“‚ Loading file: D:\...\test_file.pdf
ğŸ–¼ï¸  Parsing PDF using MinerU...
ğŸ“„  Markdown file detected: output/test_file/auto/test_file.md
ğŸ§¹  Markdown cleaning completed.
ğŸ“–  Extracted and cleaned text length: 48,372 characters
âœ‚ï¸  Initializing semantic chunker...
âœ…  Local semantic chunker loaded: sentence-transformers/all-MiniLM-L6-v2
ğŸ§   Performing semantic segmentation...
ğŸ¯  Completed â€” 42 chunks generated.
ğŸ’¾  Saving results...
âœ…  Output saved to: output/test_file/test_file.all_chunks.txt
```

---

## ğŸ“œ License

This project is licensed under the **MIT License**.  
You are free to use, modify, and distribute this software, provided that the original copyright notice and this license are included in all copies or substantial portions of the Software.

> For more details, see the [LICENSE](./LICENSE) file.

---

## ğŸ¤ Contributing

Contributions are welcome!  
If youâ€™d like to improve functionality, fix bugs, or extend compatibility (e.g., support for other file types or embedding models), please feel free to open a pull request or create an issue.

---

## ğŸ“« Contact

For questions or collaboration opportunities, please contact the project maintainer.

---

Â© 2025 Shenzhen Longgang District Data Co., Ltd. - Data Asset Management Center - Computational Modeling Department.
Developed for research and educational purposes.
# #
